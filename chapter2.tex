% chap2.tex (Definitions)



\chapter{Literature Review}\label{background}

This section discusses the state of the art in data provenance collection systems, pruning techniques for data provenance, and provenance-based intrusion detection systems.


\section{Data Provenance Collection Systems}

There has been a considerable amount of work done on data provenance collection \cite{ bates_trustworthy_2015, gessiou_towards_2012, _general-purpose_2012, muniswamy_reddy_provenance_2010}. Some of the work done has been focused on databases, sensor networks, scientific workflow systems and file system provenance but so far little attention has been given to provenance in CPS. Some of the prior work done on data provenance collection are outlined as follows:



%System call provenance collection is further divided into kernel space provenance collection and user space provenance collection. Kernel involves provenance collection of system events such as files and system call. User space on the other

\subsubsection{Kernel-space provenance collection}

Muniswamy-Reddy
et al. \cite{muniswamy_reddy} developed PASS, a provenance collection system that tracks  system-level provenance of the Linux file system. Provenance information
is stored in the same location as the file system for easy accessibility, backup,
restoration, and data management. Provenance information is collected and stored in
the kernel space. PASS is composed of 3 major components: provenance collector, provenance storage, and provenance query. The collector keeps track of system level provenance. It intercepts system calls which are translated into provenance data and initially stored in memory as inode cache. Provenance data is then transferred to a file system in an kernel database, BerkleyDB. This database maps key value pairs for provenance data for fast index look up. PASS collects and stores provenance information containing a reference to the executable that created the provenance data, input files, a description of hardware in which provenance data is produced, OS information, process environment, process parameters, and other data such as a random number generator seed. PASS detects and eliminates cycles that might occur in provenance dependencies as a result of version avoidance. Cycles violate the dependency relationships between entities. For example, a child node could depend on a parent node and also be an ancestor of a parent node. PASS eliminates cycles by merging processes that might have led to the cycles. It also provides functionality for querying provenance data in the database. The query tool is built on top of BerkleyDB. For querying provenance, users can process queries using the provenance explorer. Query is done using commands such as MAKEFILE  GENERATION which creates the sequence of events that led to the final state of a file or process. DUMP ALL, gets the provenance of the requested file. Our approach addresses application level provenance for memory constrained embedded systems without requiring OS modification.


Pohly et al. \cite{hi_fi}  developed Hi-Fi, a system level provenance collection framework for the Linux kernel using Linux Provenance Modules (LPM), this framework tracks system level provenance such as interprocess communication, networking, and kernel activities. This is achieved by mediating access to kernel objects using Linux Security Model (LSM) which is a framework that was designed for providing custom access control into the Linux kernel. It consists of a set of hooks which executed before access decision is made. LSM was designed to avoid problem created by direct system call interception. The provenance information collected from the kernel space is securely transmitted to the provenance recorder in the user space. 
\par HiFi contains three components: provenance collector, provenance log and provenance handler. The collector and log are contained in the kernel space while the handler is contained in the user space. The log is a storage medium which transmits the provenance data to the user space. The collector uses LSM which resides in the kernel space. The collector records provenance data and writes it to the provenance log. The handler reads the provenance record from the log. This approach to collecting provenance data differs from our work since we focus on embedded systems and are concerned with input and output (I/O) data, which involves sensor and actuator readings. Additionally,  HiFi deals with collecting system level events which might incur additional overhead when compared to collecting application level provenance. HiFi is engineered to work solely on the Linux operating system. Embedded systems that do not run on Linux OS will not be able to incorporate HiFi. 

Bates et al. \cite{bates_towards_2013} provide a security model for provenance collection in the linux kernel. This is achieved by creating a Linux Provenance Model (LPM). LPM serves as a security layer which provides a security abstraction for provenance data. It is involved in the attestation disclosure of the application layer and authentication channel for the network layer. The goal of LPM is to provide an end to end provenance capture system. LPM ensures the following security guarantees: For LPM, the system must be able to detect and avoid malicious forgery of provenance data. The system must be trusted and easily verifiable. \par When an application invokes a system call, the LPM tracks system level provenance which is transmitted to the provenance module via a buffer. The provenance module registers contain hooks which records system call events. These events are sent to a provenance recorder in the user space. The provenance recorder ensures that the provenance data is stored in the appropriate backend of choice. Provenance recorders offer storage support for Gzip, PostGreSQL, Neo4j and SNAP. This approach differs from our approach since it deals with provenance collection of system call events in the kernel space.

King and Chen \cite{King:2003:BI:945445.945467} developed a system, Backtracker, which generates an information flow of OS objects (e.g file, process) and events (e.g system call) in a computer system for the purpose of intrusion detection. From a detection point, information can be traced to pinpoint where a malicious attack occurred. The information flow represents a dependency graph which illustrates the relationship between system events. Detection point is a point in which an intrusion was discovered. Time is included in the dependency between objects to reduce false dependencies. Time is denoted by an increasing counter. The time is recorded as the difference of when a system call is invoked till when it ends. Backtracker is composed of two major components: EventLogger and GraphGen. An EventLogger is responsible for generating system level event log for applications running on the system. EventLogger is implemented in two ways: using a virtual machine and also as a stand alone system. In a virtual machine, the application and OS is run within a virtual machine. The OS running inside of the virtual machine is known as the guest OS while the OS on the bare-metal machine is known as the host OS, hypervisor or virtual machine monitor. The virtual machine alerts the EventLogger in the event that an application makes a system call and or exits. EventLogger gets event information, object identities, dependency relationship between events from the virtual machine's monitor and also the virtual machine's physical memory. EventLogger stores the collected information as a compressed file. EventLogger can also be implemented as a stand alone system which is incorporated in an operating system. Virtual machine is preferred to a standalone system because of the use of virtual machine allows ReVirt to be leveraged. ReVirt enables the replay of instruction by instruction execution of virtual machines. This allows for whole information capture of workloads. After the information has been captured by the EventLogger, GraphGen is used to produce visualizations that outlines the dependencies between events and objects contained in the system. GraphGen also allows for pruning of data using regular expressions which are used to filter the dependency graph and prioritize important portions of the dependency graph. This approach deals with provenance collection of OS based system events. Our approach deals with provenance collection on application based event derived through application instrumentation on CPS devices.





\subsubsection{User-space provenance collection}
RecProv \cite{rec_prov} is a provenance system which records user-level provenance, avoiding the overhead incurred by kernel level provenance recording. It does not require changes to the kernel like most provenance monitoring systems. It uses Mozilla rr to perform deterministic record and replay by monitoring system calls  and non deterministic input. Mozilla rr is a debugging tool for Linux browser. It is developed for the deterministic recording and replaying of the Firefox browser in Linux. RecProv uses PTRACE\_PEEKDATA from ptrace to access the dereferenced address of the traced process from the registers. Mozilla rr relies on ptrace, which intercepts system calls to monitor the CPU state during and after a system call. It ptrace to access the dereferenced address of the traced process from the registers. System calls are monitored for file versioning. The provenance information generated is converted into PROV-JSON, and stored in Neo4j, a graph database for visualization and storage of provenance graphs. Our approach
focuses on event flow of application logic on memory constrained CPS devices without requiring OS level interference.


Spillance et al \cite{story} developed a user space provenance collection system, Storybook, which allows for the collection of provenance data from the user space thereby reducing performance overhead. This system takes a modular approach that allows the use of application specific extensions allowing additions such as database provenance, system provenance, and web and email servers. It achieves provenance capture intercepting system level events on FUSE, a file system and MySQL , a relational database. StoryBook allows developers to implement provenance inspectors these are custom provenance models which captures the provenance of specific applications which are often modified by different application (e.g web servers, databases). When an operation is performed on a data object, the appropriate provenance model is triggered and provenance data for that data object is captured. StoryBook stores provenance information such as open, close, read or write, application specific provenance, and causality relationship between entities contained in the provenance system. Provenance data is stored in key value pairs using Stasis and Berkeley DB as the storage backend. It also allows the use of interoperable data specifications such as RDF to transfer data between various applications. Storybook allows for provenance query by looking up an inode in the ino hashtable. In our approach to provenance collection, we are particularly interested in provenance collection of application event logic on memory constrained CPS devices.

Goshal et al \cite{ghoshal_provenance_2013} developed a framework for collecting provenance data from log files. They argue that log data contains vital information about a system and can be an important source of provenance information. Provenance is categorized based on two types: process provenance and data provenance. Process provenance involves collecting provenance information of the process in which provenance is captured. Data provenance on the other hand describes the history of data involved in the execution of a process. Provenance is collected by using a rule based approach which consists of a set of rules defined in XML. The framework consists of three major components. The rule engine which contains XML specifications for selecting, linking and remapping provenance objects from log files. The rule engine processes raw log files to structured provenance. There are three categories of rules as specified by the grammar. The match-set rules which selects provenance data based on a set of matching rules. Link rules which specifies relationship between provenance events and remap rules are used to specify an alias for a provenance object. The rule engine is integrated with the log processor. The log processor component is involved with parsing log files with information contained in the rule engine. It converts every matching log information to a provenance graph representing entities (vertices) and their relationship (edges). The adapter converts the structured provenance generated by the log processor into serialized XML format which is compatible with Karma, a provenance service application that is employed for the storage and querying of the provenance data collected. This approach differs from our approach since our approach focuses on provenance data from application event logic in real-time as opposed to post processing of log files.

\subsection{Provenance collection in Sensor Networks}
Lim et al. \cite{lim} developed a
model for calculating the trust of nodes contained in a sensor network by using data
provenance and data similarity as deciding factors to calculate trust. The value of
provenance signifies that the more similar a data value is, the higher the trust score.
Also, the more the provenance of similar values differ, the higher their trust score. The trust score of a system is affected by the trust score of the sensor that forwards data to the system. Provenance is determined by the path in which data travels through the sensor network. This work differs from our approach since the authors focus on creating a trust score
of nodes connected in a sensor network using data provenance and do not emphasize
how the provenance data is collected. We are focused on creating a
provenance aware system for application based events which are used to ensure trust of
connected devices. 

\subsection{Provenance collection for scientific experiments}

Grouth et al.~\cite{groth} developed a provenance collection system, PReServ which allows software
developers to integrate provenance recording into their applications. They introduce
P-assertion recording protocol as a way of monitoring process documentation for Service
Oriented Architecture (SOA) in the standard for grid systems. PReServ is the implementation
of P-assertion recording protocol (PreP). PreP specifies how provenance can be
recorded. PReServ contains a provenance store web service for recording and querying of
P-assertions. P-assertions are provenance data generated by actors about the application
execution. It contains information about the messages sent and received as well as state of
the message. PReServ is composed of three layers, the message translator which is involved
with converting all messages received via HTTP requests to XML format for the provenance
store layer. The message translator also determines which plug-in handle to route incoming
request to, the Plug-Ins layer. It implements functionality that the Provenance Store component
provides. This component allows third party developers to add new functionality
to store provenance data without going through details of the code implementation. The
backend component stores the P-assertions. Various backend implementations could be
attached to the system by the plug-in component.

\par PReServ was developed to address the needs of specific application domain (Scientific experiments). It deals with recording process documentation of Service Oriented Architecture (SOA) and collects P-assertions which are assertions made about the execution (i.e., messages sent and received, state information) of actions by actors. Our approach is focused on collecting provenance data on application events in CPS devices through application instrumentation.



%The provenance collected by PReServ does not demonstrate causality and dependency between objects. In other words, the provenance data collected are not represented using a provenance model (e.g PROV-DM) that depicts causal relationship between provenance data. 

\subsection{Application Instrumentation-based provenance collection}

Tariq et al. \cite{tariq_towards_2012} developed a provenance collection system which automatically collects interprocess provenance of applications source code at run time. This takes a different approach from system level provenance capture. It achieves provenance capture by using LLVM compiler framework and SPADE provenance management system. LLVM is a framework that allows dynamic compilation techniques of applications written in C, C++, Objective-C, and Java. Provenance data is collected from function entry and exit calls and is inserted during compilation. LLVM contains an LLVM reporter. This is a Java class that parses the output file collected from the LLVM reporter and forwards the provenance data collected to the SPADE tracer. SPADE is a provenance management tool that allows for the transformation of domain specific activity in provenance data. The LLVM Tracer module tracks provenance at the exit and entry of each function. The output is an Open Provenance Model in which functions are represented by an OPM process, arguments and return values are represented as an OPM artifact. To minimize provenance record, users are allowed to specify what functions they would like to collect provenance information at run time. The LLVM optimizer generates a graph of the target application. This graph is used to perform reverse reachability analysis  from the functions contained in the graph. A workflow of how the framework works in collecting provenance is as follows:

\begin{itemize}
\item The application is compiled and converted into bitcode using the LLVM C compiler, clang.

\item The LLVM Tracer module is compiled as a library.

\item LLVM is run in combination with the Tracer, passed to include provenance data. 

\item Instrumentation bitcode is converted into assembly code via the compiler llc

%\item The socket code is compiled into assembly code.


\item  The instrumented application and assembly code is linked into an executable binary and sent to the SPADE kernel.
\end{itemize}


%One major limitation to this approach of collecting application level provenance is that a user is required to have the source code of the specific application in which provenance data is required. Also, provenance collection is limited to function's exit and entry points. Provenance collection deals with data pruning by allowing a user to specify what provenance data to collect which is similar to our policy based approach for efficient provenance data storage.

In this approach, provenance collection is done at program function level during function entry and exit which might lead to collecting a lot of information that might be irrelevant. Our approach on the other hand specifies what events to collect via application instrumentation. 


Gessiou et al \cite{gessiou_towards_2012} propose a dynamic instrumentation framework for data provenance collection that is universal and does not incur the overhead of kernel or source code modifications. This is achieved by using DTrace, a dynamic instrumentation utility that allows for the instrumentation of user-level and kernel-level code and with no overhead cost when disabled. The goal is to provide an easy extension to add provenance collection on any application regardless of its size or complexity. Provenance collection is implemented on the file system using PASS and evaluated on a database (SQLite) and a web browser (Safari). 
The logging component monitors all system calls for processes involved. The logging component contains information such as system-call arguments, return value, user id, process name, and timestamp. The logging components includes functionalities to collect library and functional calls. The authors argue that applying data provenance to different layers of the software stack can provide varying levels of information. Provenance needs to be captured at the layer in which it is closest to the information that is needed to be captured. Provenance information that pertains to complex system activities such as a web browser or a database are collected. The information is collected from a process system-call and functional-call based on valid entry points contained in the process. DTrace dynamic instrumentation helps in discovering interesting paths in a system in which data propagates. This helps users use this information to collect a more accurate provenance data. Our approach differs from this approach since this approach allows for instrumentation of kernel level trace. Our approach focuses on application level event trace.


ProvThings~\cite{Wang2017FearAL}, an IoT based provenance-collection framework and API, allows application developers to generate provenance through instrumentation of devices by identifying security sources and sinks through a selective instrumentation algorithm. This framework's implementation is focused on a Samsung-based IoT platform, SmartThings. ProvThings utilizes a policy-based approach to intrusion detection in which a provenance graph pattern is matched based on defined system behavior. One distinct difference between this approach and our approach is that ProvThings is focused on API based cloud devices connected to a hub. Our approach deals with provenance collection on memory constrained CPS devices.

ContextIoT~\cite{jia2017contexiot} is an access control framework that provides contextual integrity support for IoT applications by identifying sensitive actions and run-time prompts. Contextual information is defined based on data flow of the execution code during run-time and utilizes a rule-based approach to application security by defining permissible actions to sensitive resources based on data flow. A prototype was implemented on Samsung's SmartThings IoT platform.  Our approach differs from the previously mentioned IoT-based provenance collection systems since it provides provenance collection at the device level as opposed to smart device APIs and cloud-based components.

% We also utilize an approach which reduces a provenance graph into a vector space representation to determine anomalous system events based on a defined threshold. 


\section{Provenance-based Pruning}

Bates et al \cite{Bates:2015:TOY:2814579.2814586} developed a system which allows maximizing provenance storage collection in the Linux OS environment using mandatory access control (MAC) policy. This enables avoiding the collection of unnecessary provenance data and only records interesting provenance that is important to an application. In a MAC system, every object contains a label and the MAC policy specifies interactions between different labels. Provenance policy contains security context which is enforced by the MAC policy and contains permissions based on security context. It also provides the connection between provenance and MAC. The authors argue that the system collects complete provenance without gaps in provenance relationship. This is achieved by extending the work of Vijayakumar et al. \cite{asiaccs12-vijayakumar} Integrity Walls project which allows mining SELinux policies with the aim of finding Minimal Trusted Computing Base (MTCB) of various applications. A policy document is divided into trusted and untrusted labels. The trusted labels provides a thorough description of events that can have access to the application and this information serve as a provenance policy that ensures the complete provenance relationship. This approach to pruning relies on the MAC policy scheme in the Linux OS. Our approach deals with prunning with consideration to memory constrained CPS devices which might not contain an OS. 

%
%Ali et al \cite{Ali:2013:PPL:2569433.2569893} provides a provenance model, cProv, which illustrates the relationship between entities contained in a cloud system. This model is build on PROV notation. cProv contains 5 derived nodes(cprov:cProcess, cprov:Resource, cProv:Transition, cprov:cResource, and cprov:pResource ). There are a total of 10 edges built on the relationships contained in the PROV notation. Node consists of properties such as location, time-to-live, virtual-to physical mappings, executed operations. They also develop a policy language, cProvl that allows for access of provenance data. cProvl is represented in XML and consists of rules  and conditions that expresses logic to enforce access of resources. Each rule contains an entity. XACML, a policy language for the enforcement of access control is used for implementing access control on the provenance policy information. cProvl is mapped to XACML to allow the policy to run on XACML. An example of how the policy structure works is as follows: An application makes a request, this request is converted to an appropriate provenance aware request. This request is then forwarded to the policy engine. The request is evaluated by this layer by evaluating one or more policy rules as contained in a policy. This creates a response which is forwarded to the provenance converter that converts the request to an appropriate format for client utilization.

Hussein et al. \cite{hussain_secure_2014} encode the provenance of the path in which  sensor provenance travels using arithmetic coding, a data compression algorithm. Arithmetic coding assigns intervals to a string of characters by  cumulative probabilities of each character contained in the string. It assign probabilities by monitoring the activities of the Wireless Sensor Network, this is known as the training phase. In this phase the number of times a node appears in a packet is counted. Each node in the WSN is regarded as a symbol. This is used as input to the arithmetic coding algorithm which generates a variable interval for the combination of symbols. The WSN contains a Base Station (BS) and a network of nodes. The BS is in charge of verifying the integrity of the packets sent. It is also where the encoded characters using arithmetic coding are decoded. For their application, provenance is referred to as the path in which data travels to the BS. Here, provenance is divided into two types: Simple provenance in which data generated from the leaf nodes, are forwarded to surrounding nodes towards the BS. In the other form of provenance, data is aggregated at an aggregation node and forwarded to a BS. Each node in the WSN is represented as a string of characters and is encoded using arithmetic coding. The BS is responsible for decoding encoded provenance information. Provenance is encoded at the respective nodes and forwarded to the next node in the sensor network. The BS recovers characters the same way they were encoded. It also receives the encoded interval to which it uses to decode characters by using the probability values saved in its storage.

 One limitation to this approach is that provenance is considered as the path in which data travels to get to the base station and not the data that is transmitted itself. Provenance data collected are sensor ids which are represented as single characters. In contrast to their approach, our provenance collection framework collects provenance data not just pertaining to the identification of the ``who" provenance characteristics but also other components of provenance. 
 
 Another common approach looks at ways to compress provenance graphs. Xie et al.~\cite{xie11-tapp}  adapt a modified web compression technique for provenance graphs based on locality, similarity, and consecutiveness of nodes contained in the provenance graph.  Wang et al.~\cite{7038199} proposed a dictionary based compression technique for provenance graph compression.  Xie et al.~\cite{Xie:2012:HAE:2396761.2398511} proposed a hybrid approach which consists of web and a dictionary compression technique. This approach focuses on compression as a technique to storage optimization. Our approach employs the use of data pruning heuristic  to eliminate provenance data that is unnecessary to our application use case. 
 
 
 
 \subsection{Provenance-based IDS}
 
PIDAS~\cite{Xie:2016:UID:2936026.2936232} is an IDS that uses provenance graphs generated from system calls, which reveals interactions between files and processes. This approach employs the use of a rule-matching technique for anomaly detection. Our approach on the other hand involves the vectorization of provenance graphs and a similarity technique to compare graphs. We also use a threshold value to classify anomalous instances. 

FlowFence~\cite{197137} is a rule-based approach to malicious intrusion detection that allows users to declare the data flow of sensitive data in an application thereby data that does not fit the predefined flow path are denied access to resources.  This approach uses a rule-based technique to anomaly detection. Our approach uses a threshold score which can be fine-tuned to determine anomalous instances. 

PANDDE~\cite{Fadolalkarim} uses OS-based provenance to track a data object downloaded from a database after which it profiles a user's behavior based on predefined actions such as data printing, emailing, and storage to detect data exfiltration attacks. This approach differs from our approach to anomaly detection since it utilizes a rule-matching technique in which a user profile is directly matched to currently executing program in the detection phase. 

FRAPpuccino~\cite{203308} is a provenance-based fault detection system that runs on a cluster of machines which detects anomalous behavior in the provenance graph based on a dynamic sliding window and clustering. One drawback of this approach is assumed notion that anomalous instances can be detected from a subset of provenance graphs thereby using sliding window which only considers a subset of the entire training and test set. FRAPpuccino shares some similarity with our approach since they utilize a window size for anomaly detection however some key differences exists. We utilize an approach to anomaly detection which reduces a provenance graph into a vector space representation and determine anomalous system events based on a defined threshold value.

Mukherjee et al.~\cite{Mukherjee2017APG} proposed a precedence graph based anomaly detection technique to detect message injection attacks in J1939 networks. Precedence graphs are used to model temporal relationships between data events. To detect anomalous instances in the precedence graphs, normalized flux capacity is used. Flux capacity is the product of in-degree and out-degree of a node. We utilize a different approach to anomaly detection. Our approach reduces provenance graphs to a vector space representation which is then compared (training and test graph set) using cosine similarity metric.

Hailesellasie and Hasan~\cite{hailesellasie_intrusion_2018} introduce a graph-based IDS for industrial controllers that relies on exact comparison of graphs derived from control logic of a programmable logic controller program. Our approach does not use exact comparison, because exact graph comparison is brittle in complex, irregular CPS applications.















%\section{Intrusion Detection for IoT}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 